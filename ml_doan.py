# -*- coding: utf-8 -*-
"""ML_DoAn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hJMLvYUiXOeOTOeIgdyFoTNb9XOv-KbQ
"""

import numpy as np
import pandas as pd
from collections import Counter
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Daegu_Real_Estate_data.csv")
# Kiem tra du lieu null
df.isnull().sum()
# Kiem tra kieu cua cac thuoc tinh
df.info()

# # Chuyen cac du lieu dang object ve dang number
subway_mapping={
    '0-5min': 4,
    '5min~10min': 3,
    '10min~15min': 2,
    '15min~20min': 1,
    'no_bus_stop_nearby': 0
}
bus_mapping={
    '0~5min': 2,
    '5min~10min': 1,
    '10min~15min': 0
}
hallayType_mapping={
    'terraced':2,
    'mixed':1,
    'corridor':0
}
heatingType_mapping={
    'individual_heating':1,
    'central_heating':0
}
aptManageType_mapping={
    'management_in_trust':1,
    'self_management':0,
}
subwayStation_mapping={
    'Kyungbuk_uni_hospital':2,
    'Myung-duk':1,
    'Banwoldang':1,
    'Bangoge':1,
    'Sin-nam':1,
    'no_subway_nearby':0,
    'Chil-sung-market':2,
    'Daegu':1
}
# Ap dung cac thay doi tren vao data
df['TimeToSubway']=df['TimeToSubway'].map(subway_mapping)
df['TimeToBusStop']=df['TimeToBusStop'].map(bus_mapping)
df['HallwayType']=df['HallwayType'].map(hallayType_mapping)
df['HeatingType']=df['HeatingType'].map(heatingType_mapping)
df['AptManageType']=df['AptManageType'].map(aptManageType_mapping)
df['SubwayStation']=df['SubwayStation'].map(subwayStation_mapping)
# df.info()


# Phat hien du lieu outlier(ngoai lai) cua cot sale
plt.figure(figsize=(20,8))
# sns.distplot(df['SalePrice'])
sns.boxplot(y=df['SalePrice'])
# sort cot SalePrice va lay index cua cac SalePrice > 510000
df.sort_values(by='SalePrice').loc[df['SalePrice']>510000]
print('-----Delete outlier: %d'%(len(df.sort_values(by='SalePrice').loc[df['SalePrice']>510000])))

# Loai bo cac dong du lieu co SalePrice > 510000
adj_df = df.drop(df.loc[df['SalePrice']>510000].index, axis=0)


# Chuan hoa du lieu
from sklearn import preprocessing
X = adj_df.iloc[:, 1:].values
y= adj_df.iloc[:, 0].values
stdsc = preprocessing.StandardScaler()
X_std = stdsc.fit_transform(X)
# print(adj_df.shape)

#Ap dung cac thuat toan de tao model
scoresOfLinearRegression=[] #Luu ket qua danh gia cua thuat toan Linear Regression
scoresOfDecisionTree=[[], [], [], [], []] #Luu ket qua danh gia cua thuat toan Decision Tree
scoresOfGBR=[] # Luu ket qua danh gia cua Gradient Boosting Regression

# Linear Regression
from sklearn import linear_model
def linearRegressionFunc(X_train, y_train, X_test, y_test):
  lr=linear_model.LinearRegression()
  lr.fit(X_train, y_train)
  y_pred=lr.predict(X_test)
  scoresOfLinearRegression.append({
      'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
      'MAE': mean_absolute_error(y_test, y_pred),
      'R2 score': lr.score(X_test, y_test)})

# Decision Tree
from sklearn.tree import DecisionTreeRegressor
def decisionTreeRegressorFunc(X_train, y_train, X_test, y_test):
  for i in range(11, 16):
    dt=DecisionTreeRegressor(max_depth=i)
    dt.fit(X_train, y_train)
    y_pred=dt.predict(X_test)
    scoresOfDecisionTree[i-11].append({
      'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
      'MAE': mean_absolute_error(y_test, y_pred),
      'R2 score': dt.score(X_test, y_test)})

# GBR
from sklearn import ensemble
def GBR(X_train, y_train, X_test, y_test):
  params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,
          'learning_rate': 0.01, 'loss': 'ls'}
  clf = ensemble.GradientBoostingRegressor(**params)
  clf.fit(X_train, y_train)
  y_pred=clf.predict(X_test)
  scoresOfGBR.append({
      'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
      'MAE': mean_absolute_error(y_test, y_pred),
      'R2 score': clf.score(X_test, y_test)})
  

from sklearn.model_selection import train_test_split
# Kiem tra do chinh xac 10 lan va tinh trung binh
for i in range(1, 11):
  # Chia tap train va test
  X_train, X_test, y_train, y_test=train_test_split(X_std, y, test_size=0.3, random_state=i)
  linearRegressionFunc(X_train, y_train, X_test, y_test)
  decisionTreeRegressorFunc(X_train, y_train, X_test, y_test)
  GBR(X_train, y_train, X_test, y_test)

# Danh gia Linear Regression
lrRMSE=sum([item['RMSE'] for item in scoresOfLinearRegression])/len(scoresOfLinearRegression)
lrMAE=sum([item['MAE'] for item in scoresOfLinearRegression])/len(scoresOfLinearRegression)
lrR2Score=sum([item['R2 score'] for item in scoresOfLinearRegression])/len(scoresOfLinearRegression)
print('-----------------Linear Regression------------------------')
print('RMSE: %.2f, MAE: %.2f, R2 score: %.2f' %(lrRMSE, lrMAE, lrR2Score))

# Danh gia Decision Tree
tags=['RMSE', 'MAE', 'R2 score']
print('-----------------Decision Tree------------------------')
for i in range(0, 5):
  for tag in tags:
    print('%s with max_depth = %d' %(tag, i+11))
    print('%.2f' %(sum([item[tag] for item in scoresOfDecisionTree[i]])/len(scoresOfDecisionTree[i])))

# Danh gia Gradient Boosting Regression
gbrRMSE=sum([item['RMSE'] for item in scoresOfGBR])/len(scoresOfGBR)
gbrMAE=sum([item['MAE'] for item in scoresOfGBR])/len(scoresOfGBR)
gbrR2Score=sum([item['R2 score'] for item in scoresOfGBR])/len(scoresOfGBR)
print('-----------------Gradient Boosting Regression------------------------')
print('RMSE: %.2f, MAE: %.2f, R2 score: %.2f' %(gbrRMSE, gbrMAE, gbrR2Score))

# Tim thuoc tinh quan trong bang Gradient Boosting Regression
X_train, X_test, y_train, y_test=train_test_split(X_std, y, test_size=0.3, random_state=17)
params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2, 'learning_rate': 0.01, 'loss': 'ls'}
# Tao model voi cac parament tren
clf = ensemble.GradientBoostingRegressor(**params)
clf.fit(X_train, y_train)
# Lay cac gia tri thuoc tinh quan trong
feature_importance = clf.feature_importances_
feature_importance = 100.0 * (feature_importance / feature_importance.max())
# Tra ve index cua cac gia tri sau khi sap xep
sorted_idx = np.argsort(feature_importance)
pos = np.arange(sorted_idx.shape[0])
plt.subplots(figsize=(15, 10))
plt.subplot(1, 2, 2)
adj_df.drop('SalePrice', axis=1, inplace=True)
plt.barh(pos, feature_importance[sorted_idx], align='center')
# Gan nhan cac thuoc tinh cho cot y
plt.yticks(pos, adj_df.columns[sorted_idx])
plt.title('Variable Importance')
plt.show()


for index, value in enumerate(scoresOfGBR, start=1):
  print('Scores in loop %.2f of GBR' %(index))
  print(value)

"""* Tiền xử lí
  1.   Kiểm tra tập dữ liệu có null hay không?
  2.   Kiểm tra kiểu của các thuộc tính trong tập dữ liệu. Phát hiện có 6 thuộc tính dạng object.
  3.   Tiến hành chuyển đổi dữ liệu dạng object sang kiểu int
  4.   Phát hiện dữ liệu outlier. Phát hiện SalePrice có outlier. Hình đầu tiên. Vào link này để biết thêm outlier là gì và cách tìm bằng boxplot
          https://towardsdatascience.com/boxplot-for-anomaly-detection-9eac783382fd
  5.   Chuẩn hóa dữ liệu. Vào link này để đọc thêm.
          https://machinelearningcoban.com/general/2017/02/06/featureengineering/#standardization
* Tạo model và đánh giá độ chính xác
  1.   Chia tập train và test tỉ lệ 7/3
  2.   Tạo model với Linear Regression và đánh giá bằng 3 chỉ số: RMSE, MAE, R2 score. Ta thu được kết quả: RMSE: 36293.00, MAE: 27172.87, R2 score: 0.88. Vào link để biết các độ đo đó là gì. https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d
  3. Tạo model bằng Decision Tree với tham số max_depth từ 11->15. Thu được kết quả như trên.
  4. Tạo model bằng Gradient Boosting Regression thu được kết quả RMSE: 18133.16, MAE: 12856.97, R2 score: 0.97
* Tìm thuộc tính quan trọng
  1.   Sử dụng Gradient Bootsing Regession để tạo model
  2.   Nhìn vào biểu đồ ta thấy được thuộc tính size là thuộc tính quan trọng nhất(có ảnh hưởng lớn đến giá nhà)

**Gradient Boosting Regression:**
https://dothanhblog.wordpress.com/2020/03/03/ensemble-learning/
https://noron.vn/post/y-tuong-xgboost-la-gi-tai-sao-trong-cac-bai-toan--40ww17x4bjri

**Tham khảo**
https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.barh.html
**Nhóm mình chọn ngay tập dữ liệu hơi khó(có nhiều dữ liệu ko hợp lệ, thừa) và đây cũng là bài toán Hồi qui trong lớp cô dạy chỉ 1 thuật toán là Linear Regression và một ít của Decision Tree. Nên tui có xài thêm Gradient Boosting Descent. Mọi người cố gắng tìm hiểu thêm nha. Fighting everyone :D**



          








"""